{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92eb6227",
   "metadata": {},
   "source": [
    "# BERTopic Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de05e91",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate topic modelling and related concepts using [BERTopic](https://maartengr.github.io/BERTopic/index.html)\n",
    "\n",
    "BERTopic is a widely using topic-modelling library that follows a modularizable five-step process, making it flexible and useful for many different topic-modelling circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39620b",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6755bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e318f",
   "metadata": {},
   "source": [
    "We will use the [stanfordnlp/web_questions](https://huggingface.co/datasets/stanfordnlp/web_questions) dataset for our modelling. \n",
    "\n",
    "This dataset consists of a set of questions and answers sourced from web forums circa 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfef316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"stanfordnlp/web_questions\")\n",
    "\n",
    "train = pd.DataFrame(data[\"train\"])\n",
    "test = pd.DataFrame(data[\"test\"])\n",
    "\n",
    "#concatenate comma-separated answers into single string\n",
    "train[\"answers\"] = train[\"answers\"].apply(lambda x: ','.join(x))\n",
    "test[\"answers\"] = test[\"answers\"].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fd04a",
   "metadata": {},
   "source": [
    "The BERTopic API requires a list of strings as training data.\n",
    "\n",
    "Let's concatenate the `url`, `question`, and `answers` columns into a single `document` column for use as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"document\"] = train.apply(lambda row: f\"{row[\"url\"]} Q: {row[\"question\"]} A: {row[\"answers\"]}\", axis=1)\n",
    "test[\"document\"] = test.apply(lambda row: f\"{row[\"url\"]} Q: {row[\"question\"]} A: {row[\"answers\"]}\", axis=1)\n",
    "\n",
    "train[\"document\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a122a0",
   "metadata": {},
   "source": [
    "## BERTopic Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9907d15",
   "metadata": {},
   "source": [
    "The BERTopic pipeline is composed of five steps:\n",
    "1. **Embedding** - transform the raw text into floating point numbers that the computer can operate on\n",
    "2. **Dimensionality Reduction** - Try to represent the floating point arrays in a smaller size so that they take up less memory and are easier to work with\n",
    "3. **Clustering** - Group the embedded documents by similarities in semantics, text, etc. This will help us discover patterns in the training data.\n",
    "4. **Tokenize** - Split up longer texts into chunks i.e. tokens. The chunking (tokenization) strategy used here affects how fine-grained the topic results are.\n",
    "5. **Representation**  - Create a human-friendly display of the shared topics that were discovered in the text. By default this makes keywords.\n",
    "\n",
    "Each step has multiple choices of algorithm; for example, swapping out one dimensionality reduction algorithm for another might result in different end results. For the purpose of this demonstration I'll use the default algorithms to keep it simple.\n",
    "\n",
    "Let's walk through each step of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91d145",
   "metadata": {},
   "source": [
    "**Note**: The algorithms used here are all CPU based and can work without a GPU. For this demonstration, I'll use CPU versions of the algorithms so that this notebook is portable across environments. In the event that a GPU is available, the code can be modified to use GPU acceleration by following the instructions in the [BERTopic documentation here](https://maartengr.github.io/BERTopic/faq.html#can-i-use-the-gpu-to-speed-up-the-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ce748",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Convert the text into floating-point numerical arrays. By converting text into numbers we can apply mathematical operations and discover mathematical patterns within the embeddings. These mathematical patterns and similarities will ultimately be used to group documents by topic.\n",
    "\n",
    "The `SentenceTransformers` library is a good default here, balancing speed and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12028e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9a716",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "The embeddings created in the previous step can result in high-dimension matrices that have difficulty fitting into memory and affect performance. Dimensionality reduction algorithms like `UMAP()` can shrink the size of the matrices while maintaining most of the embedded information.\n",
    "\n",
    "The most important parameter in `UMAP()` is `n_neighbors` which controls how many surrounding data points to use when estimating the structure of the data. Higher values create a more global view of the data, at the cost of local detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d534a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cbbe5",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "This is the core step in the pipeline, because this step is the one that starts grouping documents by shared topic. These documents groups (clusters) are what the topic representation step will use as a basis to extract keywords/topics. \n",
    "\n",
    "The three most important parameters for `HDBSCAN()` are `min_cluster_size`, `min_samples`, and `cluster_selection_method`. These values affect how large and spread-out the resulting clusters are.\n",
    "\n",
    "Higher values of `min_cluster_size` will create larger clusters that cover more documents but have more general high-level topics.\n",
    "The value of `min_samples` defaults to `min_cluster_size` but can be set independently to further alter the resulting clusters.\n",
    "The `cluster_selection_method` has multiple options, but the two most commonly used are `eom` and `leaf`. `eom` has a tendency to create one or two large clusters and several smaller clusters, while `leaf` tries to make evenly sized clusters. \n",
    "\n",
    "Experiment with combinations of the three parameters to see what fits your use case.\n",
    "\n",
    "You can read more about the three parameters and their effects here:<br>\n",
    "[min_samples and min_cluster_size](https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan)<br>\n",
    "[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43715160",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    min_samples=15,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1cca8",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "This will tokenize (that is, chunk) texts. \n",
    "\n",
    "The most important parameter for `CountVectorizer()` is `ngram_range` which sets how many words each token has; choose a value that makes sense for your data. For example, if you are interested in finding instances of \"steak restaurant\" in your data, `ngram_range` should allow for tokens up to 2 words long, since \"steak restaurant\" has two words.\n",
    "\n",
    "`stop_words` are words that appear frequently in your documents and you want to ignore. By default it uses the `sklearn` english stopwords, [but those have issues](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words), so you are advised to use another set of stopwords. [NLTK stopwords are a common alternative](https://www.geeksforgeeks.org/nlp/removing-stop-words-nltk-python/).\n",
    "\n",
    "[You can read more about the vectorizer parameters and algorithms here](https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc01dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(ngram_range=(1,3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eba8b2",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "This step examines each cluster and extracts keywords that are representative of each cluster. These keywords are used to generate the topics for each cluster. \n",
    "\n",
    "The default c-TF-IDF algorithm is an innovative algorithm created by the BERTopic developers, and the distinguishing feature of the library. c-TF-IDF differs from standard TF-IDF by treating each cluster of documents as a single merged document and performing TF-IDF on that document. The most frequent terms in that merged document would then be the same as the topics.\n",
    "\n",
    "The documentation explains it well: _\"When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of **topics**! This model is called **class-based TF-IDF**\"_\n",
    "\n",
    "There are two boolean parameters that affect how much BERTopic focuses on stopwords and ignores over-frequent words, `bm25_weighting` and `reduce_frequent_words` [(documentation)](https://maartengr.github.io/BERTopic/getting_started/ctfidf/ctfidf.html#parameters). `bm25_weighting` increases the weight of stopwords such that they are more likely to be ignored when generating keywords. `reduce_frequent_words` does just as it says, in that if a word appears too frequently it will be treated as noise instead of as a keyword.\n",
    "\n",
    "[You can read more about the c-TF-IDF algorithm here](https://maartengr.github.io/BERTopic/algorithm/algorithm.html#5-topic-representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c238c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2b21f",
   "metadata": {},
   "source": [
    "Now that all the individual pipeline components have been prepared, we can begin training.\n",
    "\n",
    "Training is as simple as passing the pipeline components to the `BERTopic()` constructor and running `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b828053",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(train[\"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe425af5",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Now that we've clustered document together, let's see what distinct topics BERTopic found. There are several visualization methods available, all with their own level of detail and perspective. \n",
    "\n",
    "[You can read more about the different visualizations here](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html)\n",
    "\n",
    "I will demonstrate two visualizations: Intertopic Distance and Hiearchical Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2435dba",
   "metadata": {},
   "source": [
    "### Intertopic Distance\n",
    "\n",
    "This is a commonly used visualization, it lets you see how big each cluster is and how far apart, or distinct, each topic cluster is from others.\n",
    "\n",
    "Based on the results of this visualization you can determine if the topics are as distinct as you want them to be, and if the clusters are as fine-grained or general as you would like. This graph can help you visualize the effects of altering the clustering or dimensionality reduction parameters.\n",
    "\n",
    "[Documentation](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html#visualize-topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaaa185",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f2c98",
   "metadata": {},
   "source": [
    "### Hiearchical Labelling\n",
    "\n",
    "Topics can often be divided into overarching parent topics and child topics. Hiearchical labelling can help us visualize the parent-child relationship between topics.\n",
    "\n",
    "[Documentation](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_hierarchy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(train[\"document\"])\n",
    "\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1be5cc",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Most datasets have some documents that do not fit neatly into any topic, these documents are called outliers. Outliers are natural and forcing outliers into clusters risks introducing noise. \n",
    "\n",
    "That being said, sometimes you really do need to reduce outliers. BERTopic has an aptly named `reduce_outliers()` function for this very purpose.\n",
    "\n",
    "`reduce_outliers()` has four main strategies - probabilities, topic distributions, c-TF-IDF, and embeddings. [You can read about each strategy's characteristics here](https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388c490",
   "metadata": {},
   "source": [
    "### Counting Outliers\n",
    "\n",
    "Before we begin outlier reduction, it helps to see how many outliers there are. The -1 topic is the outlier topic, you can see the number of documents in this topic under the `Count` column. As you can see from the `Representation` column, the keywords are unrelated and random since it's a catch-all topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94347546",
   "metadata": {},
   "source": [
    "### Outlier Reduction\n",
    "\n",
    "Now that we've counted our outliers, let's reduce them. The c-TF-IDF strategy is the default, and works well enough for most use cases, so let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(train[\"document\"], topics, strategy=\"c-tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c748a5",
   "metadata": {},
   "source": [
    "### Update Topics\n",
    "\n",
    "After reducing outliers, we need to update the topic representations to account for these larger clusters. \n",
    "\n",
    "**Do not use topic reduction or topic merging techniques after updating topics, or else it will interfere with topic mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01162a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(train[\"document\"], topics=new_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2ddbc",
   "metadata": {},
   "source": [
    "### Visualize Updated Topics\n",
    "\n",
    "After reducing outliers and updating topics, the -1 topic has disappeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a89550",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The fitted topic model can also predict the topics of never-before-seen documents using the `transform()` method.\n",
    "\n",
    "Keep in mind that even after outlier reduction, some documents might still be classified as outliers during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94c71d",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e221df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.transform(test[\"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04119700",
   "metadata": {},
   "source": [
    "### Predicted Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(topics)\n",
    "\n",
    "for i in range(n):\n",
    "    topic_name = ''.join(topic_model.get_topic_info(topics[i])[\"Name\"].values) or \"Outlier\"\n",
    "\n",
    "    print(f\"Document: {test.iloc[i]['document']}\")\n",
    "    print(f\"Predicted Topic: {topic_name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6a5a4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There's a lot more to topic-modelling and BERTopic than what is demonstrated here. Look around and see what you discover!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
