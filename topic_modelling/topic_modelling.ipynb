{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92eb6227",
   "metadata": {},
   "source": [
    "# BERTopic Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de05e91",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate topic modelling and related concepts using [BERTopic](https://maartengr.github.io/BERTopic/index.html)\n",
    "\n",
    "BERTopic is a widely using topic-modelling library that follows a modularizable five-step process, making it flexible and useful for many different topic-modelling circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39620b",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d02c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6755bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e318f",
   "metadata": {},
   "source": [
    "We will use the [stanfordnlp/web_questions](https://huggingface.co/datasets/stanfordnlp/web_questions) dataset for our modelling. \n",
    "\n",
    "This dataset consists of a set of questions and answers sourced from web forums circa 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfef316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"stanfordnlp/web_questions\")\n",
    "\n",
    "train = pd.DataFrame(data[\"train\"])\n",
    "test = pd.DataFrame(data[\"test\"])\n",
    "\n",
    "#concatenate comma-separated answers into single string\n",
    "train[\"answers\"] = train[\"answers\"].apply(lambda x: ','.join(x))\n",
    "test[\"answers\"] = test[\"answers\"].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fd04a",
   "metadata": {},
   "source": [
    "The BERTopic API requires a list of strings as training data.\n",
    "\n",
    "Let's concatenate the `url`, `question`, and `answers` columns into a single `document` column for use as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7fd32d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.freebase.com/view/en/justin_bieber Q: what is the name of justin bieber brother? A: Jazmyn Bieber,Jaxon Bieber'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"document\"] = train.apply(lambda row: f\"{row[\"url\"]} Q: {row[\"question\"]} A: {row[\"answers\"]}\", axis=1)\n",
    "test[\"document\"] = test.apply(lambda row: f\"{row[\"url\"]} Q: {row[\"question\"]} A: {row[\"answers\"]}\", axis=1)\n",
    "\n",
    "train[\"document\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a122a0",
   "metadata": {},
   "source": [
    "## BERTopic Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9907d15",
   "metadata": {},
   "source": [
    "The BERTopic pipeline is composed of five steps:\n",
    "1. **Embedding** - transform the raw text into floating point numbers that the computer can operate on\n",
    "2. **Dimensionality Reduction** - Try to represent the floating point arrays in a smaller size so that they take up less memory and are easier to work with\n",
    "3. **Clustering** - Group the embedded documents by similarities in semantics, text, etc. This will help us discover patterns in the training data.\n",
    "4. **Tokenize** - Split up longer texts into chunks i.e. tokens. The chunking (tokenization) strategy used here affects how fine-grained the topic results are.\n",
    "5. **Representation**  - Create a human-friendly display of the shared topics that were discovered in the text. By default this makes keywords.\n",
    "\n",
    "Each step has multiple choices of algorithm; for example, swapping out one dimensionality reduction algorithm for another might result in different end results. For the purpose of this demonstration I'll use the default algorithms to keep it simple.\n",
    "\n",
    "Let's walk through each step of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91d145",
   "metadata": {},
   "source": [
    "**Note**: The algorithms used here are all CPU based and can work without a GPU. For this demonstration, I'll use CPU versions of the algorithms so that this notebook is portable across environments. In the event that a GPU is available, the code can be modified to use GPU acceleration by following the instructions in the [BERTopic documentation here](https://maartengr.github.io/BERTopic/faq.html#can-i-use-the-gpu-to-speed-up-the-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ce748",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Convert the text into floating-point numerical arrays. By converting text into numbers we can apply mathematical operations and discover mathematical patterns within the embeddings. These mathematical patterns and similarities will ultimately be used to group documents by topic.\n",
    "\n",
    "The `SentenceTransformers` library is a good default here, balancing speed and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12028e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9a716",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "The embeddings created in the previous step can result in high-dimension matrices that have difficulty fitting into memory and affect performance. Dimensionality reduction algorithms like `UMAP()` can shrink the size of the matrices while maintaining most of the embedded information.\n",
    "\n",
    "The most important parameter in `UMAP()` is `n_neighbors` which controls how many surrounding data points to use when estimating the structure of the data. Higher values create a more global view of the data, at the cost of local detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d534a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cbbe5",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "This is the core step in the pipeline, because this step is the one that starts grouping documents by shared topic. These documents groups (clusters) are what the topic representation step will use as a basis to extract keywords. \n",
    "\n",
    "The three most important parameters for `HDBSCAN()` are `min_cluster_size`, `min_samples`, and `cluster_selection_method`. These values affect how large and spread-out the resulting clusters are.\n",
    "\n",
    "Higher values of `min_cluster_size` will create larger clusters that cover more documents but have more general high-level topics.\n",
    "The value of `min_samples` defaults to `min_cluster_size` but can be set independently to further alter the resulting clusters.\n",
    "The `cluster_selection_method` has multiple options, but the two most commonly used are `eom` and `leaf`. `eom` has a tendency to create a single large cluster and several smaller clusters, while `leaf` tries to make evenly sized clusters. \n",
    "\n",
    "Experiment with combinations of the three parameters to see what fits your use case.\n",
    "\n",
    "Read more about the three parameters and their effects:<br>\n",
    "[min_samples and min_cluster_size](https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan)<br>\n",
    "[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43715160",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    min_samples=15,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1cca8",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "This will tokenize (that is, chunk) texts. \n",
    "\n",
    "The most important parameter for `CountVectorizer()` is `ngram_range` which sets how many words each token has; choose a value that makes sense for your data. For example, if you are interested in finding instances of \"steak restaurant\" in your data, `ngram_range` should allow for tokens up to 2 words long, since \"steak restaurant\" has two words.\n",
    "\n",
    "`stop_words` are words that appear frequently in your documents and you want to ignore. By default it uses the `sklearn` english stopwords, [but those have issues](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words), so you are advised to use another set of stopwords. [NLTK stopwords are a common alternative](https://www.geeksforgeeks.org/nlp/removing-stop-words-nltk-python/).\n",
    "\n",
    "[You can read more about the vectorizer parameters and algorithms here](https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc01dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(ngram_range=(1,3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eba8b2",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "This step examines each cluster and extracts keywords that are representative of each cluster. These keywords are used to generate the topics for each cluster. \n",
    "\n",
    "The default c-TF-IDF algorithm is an innovative algorithm created by the BERTopic developers, and the distinguishing feature of the library. c-TF-IDF differs from standard TF-IDF by treating each cluster of documents as a single merged document and performing TF-IDF on that document. The most frequent terms in that merged document would then be the same as the topics.\n",
    "\n",
    "The documentation explains it well: _\"When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of **topics**! This model is called **class-based TF-IDF**\"_\n",
    "\n",
    "[You can read more about the c-TF-IDF algorithm here](https://maartengr.github.io/BERTopic/algorithm/algorithm.html#5-topic-representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctfidf_model = ClassTfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c238c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2b21f",
   "metadata": {},
   "source": [
    "Now that all the individual pipeline components have been prepared, we can begin training.\n",
    "\n",
    "Training is as simple as passing the pipeline components to the `BERTopic()` constructor and running `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b828053",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(train[\"document\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
